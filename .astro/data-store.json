[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.17.2","content-config-digest","81377876b1548a80","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://blog.santoshchandu.com\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true,\"allowedDomains\":[]},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false,\"svgo\":false},\"legacy\":{\"collections\":false}}","blog",["Map",11,12,50,51,88,89],"mcp-usbc-moment",{"id":11,"data":13,"body":23,"filePath":24,"digest":25,"rendered":26,"legacyId":49},{"title":14,"description":15,"pubDate":16,"tags":17,"draft":22},"Model Context Protocol: The USB-C Moment for AI Tooling","Anthropic's MCP is quietly becoming the standard for how AI agents connect to the world. Here's why it matters.",["Date","2026-02-01T00:00:00.000Z"],[18,19,20,21],"AI","MCP","Agents","Dev",false,"In late 2024, Anthropic released the Model Context Protocol (MCP) — an open standard for connecting AI models to external tools and data sources. At the time it felt like an internal tooling decision made public.\n\nEight months later, it's looking more like infrastructure.\n\n## The problem MCP solves\n\nBefore MCP, every AI integration was bespoke. You wanted Claude to read your Notion docs? Write a custom function. Connect to your database? Write an adapter. Call your internal API? Write a wrapper.\n\nEvery team solved the same problem differently, and nothing was reusable.\n\nMCP defines a **client-server protocol** where:\n- **MCP servers** expose tools, resources, and prompts to AI models\n- **MCP clients** (AI applications) connect to these servers and call their capabilities\n- The protocol is transport-agnostic (works over stdio, HTTP, WebSockets)\n\nThe result is that an MCP server written for Claude works with any MCP-compatible client. Write once, connect anywhere.\n\n```json\n// Example MCP tool definition\n{\n  \"name\": \"search_codebase\",\n  \"description\": \"Search through the repository for code patterns\",\n  \"inputSchema\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"query\": {\n        \"type\": \"string\",\n        \"description\": \"The search query\"\n      },\n      \"file_pattern\": {\n        \"type\": \"string\",\n        \"description\": \"Glob pattern to filter files (e.g., '**/*.ts')\"\n      }\n    },\n    \"required\": [\"query\"]\n  }\n}\n```\n\n## Who's building on it\n\nThe server ecosystem grew faster than expected:\n\n- **Filesystem** — read/write local files with permission controls\n- **GitHub** — search repos, read PRs, manage issues\n- **Postgres / SQLite** — query databases conversationally\n- **Slack, Linear, Notion** — company knowledge, tasks, docs\n- **Browser automation** — Puppeteer, Playwright bridges\n\nMost major dev tools either have official MCP servers or community ones that work well. If you use VS Code, Zed, or Cursor with AI features, there's a good chance MCP is under the hood.\n\n## Why this is the USB-C moment\n\nUSB-C is a good analogy because it's *boring in exactly the right way*. You don't think about USB-C. You just plug things in and they work.\n\nMCP is heading in that direction for AI tooling. The goal isn't for developers to think about the protocol — it's for them to never have to think about how their AI assistant connects to their tools again.\n\nThe interesting tension is **trust**. MCP servers run locally or remotely, and they have real permissions. A poorly configured MCP server is an attack surface. The community is still working through what a secure MCP deployment looks like at scale.\n\n## Getting started\n\nIf you want to experiment:\n\n```bash\n# Install the MCP CLI\nnpm install -g @modelcontextprotocol/cli\n\n# Run the filesystem server\nnpx @modelcontextprotocol/server-filesystem ~/projects\n\n# Connect from your MCP client\nmcp connect stdio \"npx @modelcontextprotocol/server-filesystem ~/projects\"\n```\n\nThe official docs at [modelcontextprotocol.io](https://modelcontextprotocol.io) are good and kept up to date.\n\nWatch this space. The protocol is still maturing but the momentum is real.","src/content/blog/mcp-usbc-moment.md","6133a5fcec28a6d3",{"html":27,"metadata":28},"\u003Cp>In late 2024, Anthropic released the Model Context Protocol (MCP) — an open standard for connecting AI models to external tools and data sources. At the time it felt like an internal tooling decision made public.\u003C/p>\n\u003Cp>Eight months later, it’s looking more like infrastructure.\u003C/p>\n\u003Ch2 id=\"the-problem-mcp-solves\">The problem MCP solves\u003C/h2>\n\u003Cp>Before MCP, every AI integration was bespoke. You wanted Claude to read your Notion docs? Write a custom function. Connect to your database? Write an adapter. Call your internal API? Write a wrapper.\u003C/p>\n\u003Cp>Every team solved the same problem differently, and nothing was reusable.\u003C/p>\n\u003Cp>MCP defines a \u003Cstrong>client-server protocol\u003C/strong> where:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>MCP servers\u003C/strong> expose tools, resources, and prompts to AI models\u003C/li>\n\u003Cli>\u003Cstrong>MCP clients\u003C/strong> (AI applications) connect to these servers and call their capabilities\u003C/li>\n\u003Cli>The protocol is transport-agnostic (works over stdio, HTTP, WebSockets)\u003C/li>\n\u003C/ul>\n\u003Cp>The result is that an MCP server written for Claude works with any MCP-compatible client. Write once, connect anywhere.\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"json\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">// Example MCP tool definition\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">{\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  \"name\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"search_codebase\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  \"description\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"Search through the repository for code patterns\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  \"inputSchema\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">    \"type\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"object\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">    \"properties\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">      \"query\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        \"type\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"string\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        \"description\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"The search query\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">      },\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">      \"file_pattern\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        \"type\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"string\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        \"description\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"Glob pattern to filter files (e.g., '**/*.ts')\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">      }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    },\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">    \"required\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: [\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"query\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch2 id=\"whos-building-on-it\">Who’s building on it\u003C/h2>\n\u003Cp>The server ecosystem grew faster than expected:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Filesystem\u003C/strong> — read/write local files with permission controls\u003C/li>\n\u003Cli>\u003Cstrong>GitHub\u003C/strong> — search repos, read PRs, manage issues\u003C/li>\n\u003Cli>\u003Cstrong>Postgres / SQLite\u003C/strong> — query databases conversationally\u003C/li>\n\u003Cli>\u003Cstrong>Slack, Linear, Notion\u003C/strong> — company knowledge, tasks, docs\u003C/li>\n\u003Cli>\u003Cstrong>Browser automation\u003C/strong> — Puppeteer, Playwright bridges\u003C/li>\n\u003C/ul>\n\u003Cp>Most major dev tools either have official MCP servers or community ones that work well. If you use VS Code, Zed, or Cursor with AI features, there’s a good chance MCP is under the hood.\u003C/p>\n\u003Ch2 id=\"why-this-is-the-usb-c-moment\">Why this is the USB-C moment\u003C/h2>\n\u003Cp>USB-C is a good analogy because it’s \u003Cem>boring in exactly the right way\u003C/em>. You don’t think about USB-C. You just plug things in and they work.\u003C/p>\n\u003Cp>MCP is heading in that direction for AI tooling. The goal isn’t for developers to think about the protocol — it’s for them to never have to think about how their AI assistant connects to their tools again.\u003C/p>\n\u003Cp>The interesting tension is \u003Cstrong>trust\u003C/strong>. MCP servers run locally or remotely, and they have real permissions. A poorly configured MCP server is an attack surface. The community is still working through what a secure MCP deployment looks like at scale.\u003C/p>\n\u003Ch2 id=\"getting-started\">Getting started\u003C/h2>\n\u003Cp>If you want to experiment:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># Install the MCP CLI\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">npm\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> install\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -g\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> @modelcontextprotocol/cli\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># Run the filesystem server\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">npx\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> @modelcontextprotocol/server-filesystem\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> ~/projects\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># Connect from your MCP client\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">mcp\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> connect\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> stdio\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"npx @modelcontextprotocol/server-filesystem ~/projects\"\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>The official docs at \u003Ca href=\"https://modelcontextprotocol.io\">modelcontextprotocol.io\u003C/a> are good and kept up to date.\u003C/p>\n\u003Cp>Watch this space. The protocol is still maturing but the momentum is real.\u003C/p>",{"headings":29,"localImagePaths":43,"remoteImagePaths":44,"frontmatter":45,"imagePaths":48},[30,34,37,40],{"depth":31,"slug":32,"text":33},2,"the-problem-mcp-solves","The problem MCP solves",{"depth":31,"slug":35,"text":36},"whos-building-on-it","Who’s building on it",{"depth":31,"slug":38,"text":39},"why-this-is-the-usb-c-moment","Why this is the USB-C moment",{"depth":31,"slug":41,"text":42},"getting-started","Getting started",[],[],{"title":14,"description":15,"pubDate":46,"tags":47},["Date","2026-02-01T00:00:00.000Z"],[18,19,20,21],[],"mcp-usbc-moment.md","runtime-wars-2026",{"id":50,"data":52,"body":59,"filePath":60,"digest":61,"rendered":62,"legacyId":87},{"title":53,"description":54,"pubDate":55,"tags":56,"draft":22},"Bun 2.0, Deno 2.0 and the Runtime Wars Are Actually Interesting Now","For years Node.js had no serious competition. That's no longer true — and the competition is making everything better.",["Date","2026-01-28T00:00:00.000Z"],[21,57,58],"JavaScript","Runtimes","I've been running Bun in production for four months. Not as an experiment — as the primary runtime for a small but real API service that handles a few thousand requests per day.\n\nHere's what I can tell you: **it's fast, and the DX is genuinely better**, but the ecosystem gaps are still real and will bite you if you're not careful.\n\nLet me give you the honest picture.\n\n## Why this moment matters\n\nFor most of JavaScript's history on the server, Node.js was the only serious option. Ryan Dahl, who created Node, eventually regretted several of its design decisions — enough that he built Deno from scratch to fix them. Then Jarred Sumner looked at Deno and Node and decided to rebuild from scratch again, optimizing for raw speed.\n\nThe result is a Cambrian explosion that's actually healthy for the ecosystem. Competition is forcing Node to move faster. The v22 and v23 releases have been notably more aggressive than anything we saw in the v18-v20 era.\n\n## Bun 2.0: what changed\n\nBun's big unlocks in 2.0:\n\n**The bundler matured.** The `bun build` pipeline is now fast enough and compatible enough that I've replaced esbuild in several projects. Not in all — esbuild's plugin ecosystem is still richer — but for standard TypeScript + JSX bundles, Bun wins on speed.\n\n**`bun:sqlite` is genuinely great.** Synchronous, zero-dependency SQLite bindings that are faster than better-known alternatives. For small services that don't need Postgres, this is a real unlock.\n\n**Compatibility improved substantially.** The Node.js compatibility layer now handles the vast majority of npm packages without shimming. `node:crypto`, `node:stream`, `node:fs` — mostly solid now.\n\n```bash\n# Install and run in one command\nbun add hono\nbun run server.ts\n# ~50ms cold start vs ~300ms with ts-node\n```\n\n## Deno 2.0: the pivot worked\n\nDeno 2.0 made a bet that I thought was risky: full npm compatibility. The original Deno philosophy rejected npm entirely in favor of URL-based imports.\n\nThey blinked. And it was the right call.\n\nDeno 2.0 with npm compatibility means you can use the existing ecosystem while getting Deno's genuine advantages: built-in TypeScript, built-in formatter/linter, `deno.json` configuration that actually makes sense, and the permission model that stops scripts from reading your entire filesystem.\n\nThe permission model is underrated for security-sensitive contexts. `deno run --allow-net=api.stripe.com script.ts` — that script can *only* make network calls to Stripe. Nothing else. That's a real security primitive that Node doesn't have.\n\n## What Node.js got right in response\n\nNode didn't sit still:\n\n- `--experimental-strip-types` in v22 lets you run TypeScript files directly without a build step (with caveats)\n- The permission model (experimental) mirrors Deno's approach\n- The built-in test runner (`node:test`) is now good enough that I've stopped reaching for Vitest on simpler projects\n\n## My take for 2026\n\nDon't rewrite your production Node.js apps to Bun or Deno unless you have a specific reason. But for *new* projects:\n\n- **Bun** for APIs and CLIs where raw speed and startup time matter\n- **Deno** for scripts, automation, and anything where you value the security model or the integrated toolchain\n- **Node** when ecosystem compatibility is paramount or you're on a team that doesn't want to manage unfamiliar tooling\n\nThe runtime wars are making JavaScript a better place to live. That's the story, really.","src/content/blog/runtime-wars-2026.md","53579ca41e47afd3",{"html":63,"metadata":64},"\u003Cp>I’ve been running Bun in production for four months. Not as an experiment — as the primary runtime for a small but real API service that handles a few thousand requests per day.\u003C/p>\n\u003Cp>Here’s what I can tell you: \u003Cstrong>it’s fast, and the DX is genuinely better\u003C/strong>, but the ecosystem gaps are still real and will bite you if you’re not careful.\u003C/p>\n\u003Cp>Let me give you the honest picture.\u003C/p>\n\u003Ch2 id=\"why-this-moment-matters\">Why this moment matters\u003C/h2>\n\u003Cp>For most of JavaScript’s history on the server, Node.js was the only serious option. Ryan Dahl, who created Node, eventually regretted several of its design decisions — enough that he built Deno from scratch to fix them. Then Jarred Sumner looked at Deno and Node and decided to rebuild from scratch again, optimizing for raw speed.\u003C/p>\n\u003Cp>The result is a Cambrian explosion that’s actually healthy for the ecosystem. Competition is forcing Node to move faster. The v22 and v23 releases have been notably more aggressive than anything we saw in the v18-v20 era.\u003C/p>\n\u003Ch2 id=\"bun-20-what-changed\">Bun 2.0: what changed\u003C/h2>\n\u003Cp>Bun’s big unlocks in 2.0:\u003C/p>\n\u003Cp>\u003Cstrong>The bundler matured.\u003C/strong> The \u003Ccode>bun build\u003C/code> pipeline is now fast enough and compatible enough that I’ve replaced esbuild in several projects. Not in all — esbuild’s plugin ecosystem is still richer — but for standard TypeScript + JSX bundles, Bun wins on speed.\u003C/p>\n\u003Cp>\u003Cstrong>\u003Ccode>bun:sqlite\u003C/code> is genuinely great.\u003C/strong> Synchronous, zero-dependency SQLite bindings that are faster than better-known alternatives. For small services that don’t need Postgres, this is a real unlock.\u003C/p>\n\u003Cp>\u003Cstrong>Compatibility improved substantially.\u003C/strong> The Node.js compatibility layer now handles the vast majority of npm packages without shimming. \u003Ccode>node:crypto\u003C/code>, \u003Ccode>node:stream\u003C/code>, \u003Ccode>node:fs\u003C/code> — mostly solid now.\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># Install and run in one command\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">bun\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> add\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> hono\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">bun\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> run\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> server.ts\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># ~50ms cold start vs ~300ms with ts-node\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch2 id=\"deno-20-the-pivot-worked\">Deno 2.0: the pivot worked\u003C/h2>\n\u003Cp>Deno 2.0 made a bet that I thought was risky: full npm compatibility. The original Deno philosophy rejected npm entirely in favor of URL-based imports.\u003C/p>\n\u003Cp>They blinked. And it was the right call.\u003C/p>\n\u003Cp>Deno 2.0 with npm compatibility means you can use the existing ecosystem while getting Deno’s genuine advantages: built-in TypeScript, built-in formatter/linter, \u003Ccode>deno.json\u003C/code> configuration that actually makes sense, and the permission model that stops scripts from reading your entire filesystem.\u003C/p>\n\u003Cp>The permission model is underrated for security-sensitive contexts. \u003Ccode>deno run --allow-net=api.stripe.com script.ts\u003C/code> — that script can \u003Cem>only\u003C/em> make network calls to Stripe. Nothing else. That’s a real security primitive that Node doesn’t have.\u003C/p>\n\u003Ch2 id=\"what-nodejs-got-right-in-response\">What Node.js got right in response\u003C/h2>\n\u003Cp>Node didn’t sit still:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Ccode>--experimental-strip-types\u003C/code> in v22 lets you run TypeScript files directly without a build step (with caveats)\u003C/li>\n\u003Cli>The permission model (experimental) mirrors Deno’s approach\u003C/li>\n\u003Cli>The built-in test runner (\u003Ccode>node:test\u003C/code>) is now good enough that I’ve stopped reaching for Vitest on simpler projects\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"my-take-for-2026\">My take for 2026\u003C/h2>\n\u003Cp>Don’t rewrite your production Node.js apps to Bun or Deno unless you have a specific reason. But for \u003Cem>new\u003C/em> projects:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Bun\u003C/strong> for APIs and CLIs where raw speed and startup time matter\u003C/li>\n\u003Cli>\u003Cstrong>Deno\u003C/strong> for scripts, automation, and anything where you value the security model or the integrated toolchain\u003C/li>\n\u003Cli>\u003Cstrong>Node\u003C/strong> when ecosystem compatibility is paramount or you’re on a team that doesn’t want to manage unfamiliar tooling\u003C/li>\n\u003C/ul>\n\u003Cp>The runtime wars are making JavaScript a better place to live. That’s the story, really.\u003C/p>",{"headings":65,"localImagePaths":81,"remoteImagePaths":82,"frontmatter":83,"imagePaths":86},[66,69,72,75,78],{"depth":31,"slug":67,"text":68},"why-this-moment-matters","Why this moment matters",{"depth":31,"slug":70,"text":71},"bun-20-what-changed","Bun 2.0: what changed",{"depth":31,"slug":73,"text":74},"deno-20-the-pivot-worked","Deno 2.0: the pivot worked",{"depth":31,"slug":76,"text":77},"what-nodejs-got-right-in-response","What Node.js got right in response",{"depth":31,"slug":79,"text":80},"my-take-for-2026","My take for 2026",[],[],{"title":53,"description":54,"pubDate":84,"tags":85},["Date","2026-01-28T00:00:00.000Z"],[21,57,58],[],"runtime-wars-2026.md","reasoning-models-shift",{"id":88,"data":90,"body":97,"filePath":98,"digest":99,"rendered":100,"legacyId":122},{"title":91,"description":92,"pubDate":93,"tags":94,"draft":22},"The Quiet Shift: How Reasoning Models Are Changing Everything","o3, DeepSeek R1, Gemini 2.0 Flash Thinking — what happens when LLMs stop answering and start thinking?",["Date","2026-02-10T00:00:00.000Z"],[18,95,96],"LLMs","Reasoning","Something changed in late 2025 and it wasn't loud.\n\nThere was no single headline moment, no product launch that broke Twitter for 48 hours. Instead, quietly and steadily, reasoning models became the default expectation — not the novelty. Today, if you ship a product powered by an LLM that *doesn't* do chain-of-thought reasoning under the hood, you're probably leaving performance on the table.\n\n## What \"reasoning\" actually means\n\nWhen people say reasoning models, they mean models that have been trained (or prompted, or both) to **think before they answer**. Instead of mapping input → output in one forward pass, they generate intermediate thoughts — often called a \"scratchpad\" or \"thinking trace\" — before producing a final response.\n\nThe result is striking on hard problems: math olympiad questions, multi-step logic, code debugging. The difference between a direct-answer model and a reasoning model on something like AIME (American Invitational Mathematics Examination) can be 40+ percentage points.\n\nBut what's more interesting for *product builders* is what happens on the **messy middle** — the tasks that aren't clearly hard but benefit from a little deliberation.\n\n```python\n# Example: structured extraction with reasoning\nresponse = client.messages.create(\n    model=\"claude-opus-4-6\",\n    max_tokens=8000,\n    thinking={\"type\": \"enabled\", \"budget_tokens\": 5000},\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Extract all financial figures from this earnings call transcript...\"\n    }]\n)\n```\n\n## The three players worth watching\n\n**OpenAI's o-series** (o1, o3, o4-mini) pioneered the commercial rollout of test-time compute scaling. Their insight: you can make a model smarter at inference time by spending more tokens on reasoning, without retraining.\n\n**DeepSeek R1** dropped in January 2025 and was genuinely surprising. A Chinese lab produced a reasoning model competitive with o1, trained at a fraction of the cost, and open-sourced the weights. The distilled versions run on consumer hardware. This changed the cost calculus significantly.\n\n**Google's Gemini 2.0 Flash Thinking** showed that you can bolt reasoning onto a speed-optimized model without destroying latency. Flash Thinking is fast *and* thinks. That combination mattered.\n\n## What this means for developers\n\nIf you're building on top of these models right now, a few practical notes:\n\n1. **Reasoning traces are often more valuable than the answer.** For auditable systems — legal, medical, financial — the thinking trail is the feature.\n2. **Budget your tokens.** Reasoning models consume more tokens. `budget_tokens` controls how long the model thinks. Match it to task complexity.\n3. **Don't reason about everything.** Classify your tasks first. Simple retrieval, formatting, and summarization don't benefit much from extended thinking. Reserve it for judgment calls.\n4. **Evaluate differently.** Standard benchmarks weren't designed for reasoning models. You need process-level evaluation, not just output-level.\n\n## The uncomfortable question\n\nIf models can reason, what does that do to human reasoning in workflows?\n\nThe honest answer is: we don't fully know yet. What we do know is that the developers who treat reasoning models as a collaboration layer — human sets direction, model does inference work, human evaluates — seem to be getting the best results.\n\nThe ones who treat it as a replacement for thinking are finding that the errors compound in interesting and catastrophic ways.\n\nWorth paying attention to as 2026 unfolds.","src/content/blog/reasoning-models-shift.md","5bdef8e2dceac5e2",{"html":101,"metadata":102},"\u003Cp>Something changed in late 2025 and it wasn’t loud.\u003C/p>\n\u003Cp>There was no single headline moment, no product launch that broke Twitter for 48 hours. Instead, quietly and steadily, reasoning models became the default expectation — not the novelty. Today, if you ship a product powered by an LLM that \u003Cem>doesn’t\u003C/em> do chain-of-thought reasoning under the hood, you’re probably leaving performance on the table.\u003C/p>\n\u003Ch2 id=\"what-reasoning-actually-means\">What “reasoning” actually means\u003C/h2>\n\u003Cp>When people say reasoning models, they mean models that have been trained (or prompted, or both) to \u003Cstrong>think before they answer\u003C/strong>. Instead of mapping input → output in one forward pass, they generate intermediate thoughts — often called a “scratchpad” or “thinking trace” — before producing a final response.\u003C/p>\n\u003Cp>The result is striking on hard problems: math olympiad questions, multi-step logic, code debugging. The difference between a direct-answer model and a reasoning model on something like AIME (American Invitational Mathematics Examination) can be 40+ percentage points.\u003C/p>\n\u003Cp>But what’s more interesting for \u003Cem>product builders\u003C/em> is what happens on the \u003Cstrong>messy middle\u003C/strong> — the tasks that aren’t clearly hard but benefit from a little deliberation.\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># Example: structured extraction with reasoning\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">response \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> client.messages.create(\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#FFAB70\">    model\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"claude-opus-4-6\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#FFAB70\">    max_tokens\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">8000\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#FFAB70\">    thinking\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\">{\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"type\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"enabled\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"budget_tokens\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">5000\u003C/span>\u003Cspan style=\"color:#E1E4E8\">},\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#FFAB70\">    messages\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\">[{\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">        \"role\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"user\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">        \"content\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"Extract all financial figures from this earnings call transcript...\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    }]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch2 id=\"the-three-players-worth-watching\">The three players worth watching\u003C/h2>\n\u003Cp>\u003Cstrong>OpenAI’s o-series\u003C/strong> (o1, o3, o4-mini) pioneered the commercial rollout of test-time compute scaling. Their insight: you can make a model smarter at inference time by spending more tokens on reasoning, without retraining.\u003C/p>\n\u003Cp>\u003Cstrong>DeepSeek R1\u003C/strong> dropped in January 2025 and was genuinely surprising. A Chinese lab produced a reasoning model competitive with o1, trained at a fraction of the cost, and open-sourced the weights. The distilled versions run on consumer hardware. This changed the cost calculus significantly.\u003C/p>\n\u003Cp>\u003Cstrong>Google’s Gemini 2.0 Flash Thinking\u003C/strong> showed that you can bolt reasoning onto a speed-optimized model without destroying latency. Flash Thinking is fast \u003Cem>and\u003C/em> thinks. That combination mattered.\u003C/p>\n\u003Ch2 id=\"what-this-means-for-developers\">What this means for developers\u003C/h2>\n\u003Cp>If you’re building on top of these models right now, a few practical notes:\u003C/p>\n\u003Col>\n\u003Cli>\u003Cstrong>Reasoning traces are often more valuable than the answer.\u003C/strong> For auditable systems — legal, medical, financial — the thinking trail is the feature.\u003C/li>\n\u003Cli>\u003Cstrong>Budget your tokens.\u003C/strong> Reasoning models consume more tokens. \u003Ccode>budget_tokens\u003C/code> controls how long the model thinks. Match it to task complexity.\u003C/li>\n\u003Cli>\u003Cstrong>Don’t reason about everything.\u003C/strong> Classify your tasks first. Simple retrieval, formatting, and summarization don’t benefit much from extended thinking. Reserve it for judgment calls.\u003C/li>\n\u003Cli>\u003Cstrong>Evaluate differently.\u003C/strong> Standard benchmarks weren’t designed for reasoning models. You need process-level evaluation, not just output-level.\u003C/li>\n\u003C/ol>\n\u003Ch2 id=\"the-uncomfortable-question\">The uncomfortable question\u003C/h2>\n\u003Cp>If models can reason, what does that do to human reasoning in workflows?\u003C/p>\n\u003Cp>The honest answer is: we don’t fully know yet. What we do know is that the developers who treat reasoning models as a collaboration layer — human sets direction, model does inference work, human evaluates — seem to be getting the best results.\u003C/p>\n\u003Cp>The ones who treat it as a replacement for thinking are finding that the errors compound in interesting and catastrophic ways.\u003C/p>\n\u003Cp>Worth paying attention to as 2026 unfolds.\u003C/p>",{"headings":103,"localImagePaths":116,"remoteImagePaths":117,"frontmatter":118,"imagePaths":121},[104,107,110,113],{"depth":31,"slug":105,"text":106},"what-reasoning-actually-means","What “reasoning” actually means",{"depth":31,"slug":108,"text":109},"the-three-players-worth-watching","The three players worth watching",{"depth":31,"slug":111,"text":112},"what-this-means-for-developers","What this means for developers",{"depth":31,"slug":114,"text":115},"the-uncomfortable-question","The uncomfortable question",[],[],{"title":91,"description":92,"pubDate":119,"tags":120},["Date","2026-02-10T00:00:00.000Z"],[18,95,96],[],"reasoning-models-shift.md"]